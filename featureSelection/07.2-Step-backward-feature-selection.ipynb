{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Venkatpandey/DataScience_ML/blob/main/featureSelection/07.2-Step-backward-feature-selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTGeXZox4HTr"
      },
      "source": [
        "### Step backward feature selection\n",
        "\n",
        "Step Backward Feature Selection starts by fitting a model using all features in the data set and determining its performance. \n",
        "\n",
        "Then, it trains models on all possible combinations of all features -1, and removes the feature that returns the model with the lowest performance.\n",
        "\n",
        "In the third step it trains models in all possible combinations of the features remaining from step 2 -1 feature, and removes the feature that produced the lowest performing model.\n",
        "\n",
        "The algorithm stops on a criteria determined by the user. This criteria could be that the model performance does not decrease beyond a certain threshold, or alternatively, as in the mlxtend implementation, when we reach a certain number of selected features.\n",
        "\n",
        "The evaluation metric can be the roc_auc for classification or the r squared for regression for example, and is determined by the user.\n",
        "\n",
        "Step Backward Feature Selection is called greedy, because it evaluates all possible n, and then n-1 and n-2 and so on feature combinations. Therefore, it is very computationally expensive, and sometimes, if feature space is big, even unfeasible.\n",
        "\n",
        "There is a special package in Python that implements this type of feature selection: mlxtend.\n",
        "http://rasbt.github.io/mlxtend/\n",
        "\n",
        "In the mlxtend implementation of the Step Backward Feature Selection, the stopping criteria is an arbitrarily set number of features. So the search will finish when we reach the desired number of selected features. \n",
        "\n",
        "This is somewhat arbitrary, we might be selecting a sub-opimal number of features, or likewise, a high number of features. But, by looking at the performance metric returned by the algorithm as it selects the features, we can have a view, if more features do add value, or not. \n",
        "\n",
        "\n",
        "**Note**\n",
        "If we wanted to stop the search by using another criteria, we would have to code the algorithm ourselves, unfortunately :(\n",
        "\n",
        "Here I will use the Step Backward Feature Selection algorithm from mlxtend in a classification and regression dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fsCQC4Ei4HTt"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import sys\n",
        "sys.modules['sklearn.externals.joblib'] = joblib\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, r2_score\n",
        "\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcbeMvVy4HTu"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "c8bSblUB4HTv",
        "outputId": "77f45a94-0908-4103-a010-6ec4aee641be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 109)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# load dataset\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/Venkatpandey/DataScience_ML/main/dataset/dataset_2.csv')\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u-Kee9Y-4HTv",
        "outputId": "44e1c81d-1764-4664-fed1-88957c957af0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c1f3f79a-a47c-4000-8d61-b5aa29f385fd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>var_1</th>\n",
              "      <th>var_2</th>\n",
              "      <th>var_3</th>\n",
              "      <th>var_4</th>\n",
              "      <th>var_5</th>\n",
              "      <th>var_6</th>\n",
              "      <th>var_7</th>\n",
              "      <th>var_8</th>\n",
              "      <th>var_9</th>\n",
              "      <th>var_10</th>\n",
              "      <th>var_11</th>\n",
              "      <th>var_12</th>\n",
              "      <th>var_13</th>\n",
              "      <th>var_14</th>\n",
              "      <th>var_15</th>\n",
              "      <th>var_16</th>\n",
              "      <th>var_17</th>\n",
              "      <th>var_18</th>\n",
              "      <th>var_19</th>\n",
              "      <th>var_20</th>\n",
              "      <th>var_21</th>\n",
              "      <th>var_22</th>\n",
              "      <th>var_23</th>\n",
              "      <th>var_24</th>\n",
              "      <th>var_25</th>\n",
              "      <th>var_26</th>\n",
              "      <th>var_27</th>\n",
              "      <th>var_28</th>\n",
              "      <th>var_29</th>\n",
              "      <th>var_30</th>\n",
              "      <th>var_31</th>\n",
              "      <th>var_32</th>\n",
              "      <th>var_33</th>\n",
              "      <th>var_34</th>\n",
              "      <th>var_35</th>\n",
              "      <th>var_36</th>\n",
              "      <th>var_37</th>\n",
              "      <th>var_38</th>\n",
              "      <th>var_39</th>\n",
              "      <th>var_40</th>\n",
              "      <th>...</th>\n",
              "      <th>var_70</th>\n",
              "      <th>var_71</th>\n",
              "      <th>var_72</th>\n",
              "      <th>var_73</th>\n",
              "      <th>var_74</th>\n",
              "      <th>var_75</th>\n",
              "      <th>var_76</th>\n",
              "      <th>var_77</th>\n",
              "      <th>var_78</th>\n",
              "      <th>var_79</th>\n",
              "      <th>var_80</th>\n",
              "      <th>var_81</th>\n",
              "      <th>var_82</th>\n",
              "      <th>var_83</th>\n",
              "      <th>var_84</th>\n",
              "      <th>var_85</th>\n",
              "      <th>var_86</th>\n",
              "      <th>var_87</th>\n",
              "      <th>var_88</th>\n",
              "      <th>var_89</th>\n",
              "      <th>var_90</th>\n",
              "      <th>var_91</th>\n",
              "      <th>var_92</th>\n",
              "      <th>var_93</th>\n",
              "      <th>var_94</th>\n",
              "      <th>var_95</th>\n",
              "      <th>var_96</th>\n",
              "      <th>var_97</th>\n",
              "      <th>var_98</th>\n",
              "      <th>var_99</th>\n",
              "      <th>var_100</th>\n",
              "      <th>var_101</th>\n",
              "      <th>var_102</th>\n",
              "      <th>var_103</th>\n",
              "      <th>var_104</th>\n",
              "      <th>var_105</th>\n",
              "      <th>var_106</th>\n",
              "      <th>var_107</th>\n",
              "      <th>var_108</th>\n",
              "      <th>var_109</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.532710</td>\n",
              "      <td>3.280834</td>\n",
              "      <td>17.982476</td>\n",
              "      <td>4.404259</td>\n",
              "      <td>2.349910</td>\n",
              "      <td>0.603264</td>\n",
              "      <td>2.784655</td>\n",
              "      <td>0.323146</td>\n",
              "      <td>12.009691</td>\n",
              "      <td>0.139346</td>\n",
              "      <td>5.751633</td>\n",
              "      <td>2.808895</td>\n",
              "      <td>1.244055</td>\n",
              "      <td>11.269688</td>\n",
              "      <td>15.866550</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.181500e+00</td>\n",
              "      <td>1.903910</td>\n",
              "      <td>4.667888</td>\n",
              "      <td>1.842749</td>\n",
              "      <td>5.863767</td>\n",
              "      <td>0.115498</td>\n",
              "      <td>2.398785</td>\n",
              "      <td>0.139191</td>\n",
              "      <td>11.860244</td>\n",
              "      <td>4.433561</td>\n",
              "      <td>7.135750</td>\n",
              "      <td>2.240605</td>\n",
              "      <td>3.720161</td>\n",
              "      <td>5.805012</td>\n",
              "      <td>1.308222</td>\n",
              "      <td>0.133272</td>\n",
              "      <td>5.514540</td>\n",
              "      <td>11.510708</td>\n",
              "      <td>7.534482</td>\n",
              "      <td>8.779925</td>\n",
              "      <td>6.797556</td>\n",
              "      <td>8.504757</td>\n",
              "      <td>0.188741</td>\n",
              "      <td>8.783980</td>\n",
              "      <td>...</td>\n",
              "      <td>12.866988</td>\n",
              "      <td>11.369994</td>\n",
              "      <td>1.467595</td>\n",
              "      <td>10.043070</td>\n",
              "      <td>8.174325</td>\n",
              "      <td>2.088815</td>\n",
              "      <td>0.134455</td>\n",
              "      <td>1.282842</td>\n",
              "      <td>1.262513</td>\n",
              "      <td>1.114369</td>\n",
              "      <td>1.446358</td>\n",
              "      <td>15.512397</td>\n",
              "      <td>1.820403</td>\n",
              "      <td>0.619730</td>\n",
              "      <td>0.826138</td>\n",
              "      <td>6.880270</td>\n",
              "      <td>1.680353</td>\n",
              "      <td>8.659387</td>\n",
              "      <td>10.184313</td>\n",
              "      <td>7.248146</td>\n",
              "      <td>17.065003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.044600</td>\n",
              "      <td>0.176036</td>\n",
              "      <td>9.869159</td>\n",
              "      <td>4.662407e-01</td>\n",
              "      <td>7.273476</td>\n",
              "      <td>0.623398</td>\n",
              "      <td>2.070677</td>\n",
              "      <td>1.108609</td>\n",
              "      <td>2.079066</td>\n",
              "      <td>6.748819</td>\n",
              "      <td>2.941445</td>\n",
              "      <td>18.360496</td>\n",
              "      <td>17.726613</td>\n",
              "      <td>7.774031</td>\n",
              "      <td>1.473441</td>\n",
              "      <td>1.973832</td>\n",
              "      <td>0.976806</td>\n",
              "      <td>2.541417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.821374</td>\n",
              "      <td>12.098722</td>\n",
              "      <td>13.309151</td>\n",
              "      <td>4.125599</td>\n",
              "      <td>1.045386</td>\n",
              "      <td>1.832035</td>\n",
              "      <td>1.833494</td>\n",
              "      <td>0.709090</td>\n",
              "      <td>8.652883</td>\n",
              "      <td>0.102757</td>\n",
              "      <td>8.225109</td>\n",
              "      <td>2.001220</td>\n",
              "      <td>8.081647</td>\n",
              "      <td>3.933986</td>\n",
              "      <td>14.350374</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.244384e+01</td>\n",
              "      <td>1.575456</td>\n",
              "      <td>5.275010</td>\n",
              "      <td>2.750981</td>\n",
              "      <td>3.402345</td>\n",
              "      <td>0.227527</td>\n",
              "      <td>2.502344</td>\n",
              "      <td>0.197449</td>\n",
              "      <td>12.654514</td>\n",
              "      <td>3.895271</td>\n",
              "      <td>9.230702</td>\n",
              "      <td>0.719196</td>\n",
              "      <td>3.393035</td>\n",
              "      <td>6.055243</td>\n",
              "      <td>0.926661</td>\n",
              "      <td>0.221227</td>\n",
              "      <td>7.406060</td>\n",
              "      <td>10.290955</td>\n",
              "      <td>8.075000</td>\n",
              "      <td>10.034637</td>\n",
              "      <td>6.182029</td>\n",
              "      <td>7.698029</td>\n",
              "      <td>0.295115</td>\n",
              "      <td>10.308592</td>\n",
              "      <td>...</td>\n",
              "      <td>10.477765</td>\n",
              "      <td>3.026453</td>\n",
              "      <td>1.338741</td>\n",
              "      <td>16.136215</td>\n",
              "      <td>8.659485</td>\n",
              "      <td>0.567717</td>\n",
              "      <td>0.108499</td>\n",
              "      <td>1.447928</td>\n",
              "      <td>0.583342</td>\n",
              "      <td>4.454525</td>\n",
              "      <td>3.570452</td>\n",
              "      <td>15.988817</td>\n",
              "      <td>2.628892</td>\n",
              "      <td>1.251810</td>\n",
              "      <td>2.077105</td>\n",
              "      <td>7.453729</td>\n",
              "      <td>2.173920</td>\n",
              "      <td>10.357143</td>\n",
              "      <td>13.274292</td>\n",
              "      <td>8.647012</td>\n",
              "      <td>17.143991</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.161626</td>\n",
              "      <td>0.214995</td>\n",
              "      <td>8.661069</td>\n",
              "      <td>9.585002e-01</td>\n",
              "      <td>6.475936</td>\n",
              "      <td>1.230876</td>\n",
              "      <td>2.249656</td>\n",
              "      <td>0.615216</td>\n",
              "      <td>2.479789</td>\n",
              "      <td>7.795290</td>\n",
              "      <td>3.557890</td>\n",
              "      <td>17.383378</td>\n",
              "      <td>15.193423</td>\n",
              "      <td>8.263673</td>\n",
              "      <td>1.878108</td>\n",
              "      <td>0.567939</td>\n",
              "      <td>1.018818</td>\n",
              "      <td>1.416433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.938776</td>\n",
              "      <td>7.952752</td>\n",
              "      <td>0.972671</td>\n",
              "      <td>3.459267</td>\n",
              "      <td>1.935782</td>\n",
              "      <td>0.621463</td>\n",
              "      <td>2.338139</td>\n",
              "      <td>0.344948</td>\n",
              "      <td>9.937850</td>\n",
              "      <td>11.691283</td>\n",
              "      <td>8.307318</td>\n",
              "      <td>3.239122</td>\n",
              "      <td>2.699376</td>\n",
              "      <td>10.030416</td>\n",
              "      <td>14.977220</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.636780e-07</td>\n",
              "      <td>2.605838</td>\n",
              "      <td>5.459521</td>\n",
              "      <td>3.437779</td>\n",
              "      <td>5.498281</td>\n",
              "      <td>19.800000</td>\n",
              "      <td>2.136717</td>\n",
              "      <td>19.036815</td>\n",
              "      <td>11.938497</td>\n",
              "      <td>4.378310</td>\n",
              "      <td>6.843868</td>\n",
              "      <td>1.745698</td>\n",
              "      <td>3.721307</td>\n",
              "      <td>6.339151</td>\n",
              "      <td>1.479797</td>\n",
              "      <td>18.600001</td>\n",
              "      <td>8.142160</td>\n",
              "      <td>12.575593</td>\n",
              "      <td>6.752941</td>\n",
              "      <td>6.303391</td>\n",
              "      <td>5.327748</td>\n",
              "      <td>7.559745</td>\n",
              "      <td>16.951823</td>\n",
              "      <td>7.701432</td>\n",
              "      <td>...</td>\n",
              "      <td>12.795940</td>\n",
              "      <td>3.158102</td>\n",
              "      <td>2.084452</td>\n",
              "      <td>13.596735</td>\n",
              "      <td>7.136616</td>\n",
              "      <td>3.975333</td>\n",
              "      <td>19.199999</td>\n",
              "      <td>1.035094</td>\n",
              "      <td>1.039650</td>\n",
              "      <td>2.920388</td>\n",
              "      <td>18.194969</td>\n",
              "      <td>13.878539</td>\n",
              "      <td>4.177674</td>\n",
              "      <td>0.265892</td>\n",
              "      <td>0.949150</td>\n",
              "      <td>5.501881</td>\n",
              "      <td>1.545747</td>\n",
              "      <td>6.652942</td>\n",
              "      <td>10.219311</td>\n",
              "      <td>7.350044</td>\n",
              "      <td>15.865534</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.668244</td>\n",
              "      <td>0.207304</td>\n",
              "      <td>9.591838</td>\n",
              "      <td>1.426163e+00</td>\n",
              "      <td>7.552225</td>\n",
              "      <td>0.599195</td>\n",
              "      <td>1.872145</td>\n",
              "      <td>2.111624</td>\n",
              "      <td>1.861487</td>\n",
              "      <td>6.130886</td>\n",
              "      <td>3.401064</td>\n",
              "      <td>15.850471</td>\n",
              "      <td>14.620599</td>\n",
              "      <td>6.849776</td>\n",
              "      <td>1.098210</td>\n",
              "      <td>1.959183</td>\n",
              "      <td>1.575493</td>\n",
              "      <td>1.857893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.020690</td>\n",
              "      <td>9.900544</td>\n",
              "      <td>17.869637</td>\n",
              "      <td>4.366715</td>\n",
              "      <td>1.973693</td>\n",
              "      <td>2.026012</td>\n",
              "      <td>2.853025</td>\n",
              "      <td>0.674847</td>\n",
              "      <td>11.816859</td>\n",
              "      <td>0.011151</td>\n",
              "      <td>5.769939</td>\n",
              "      <td>2.760518</td>\n",
              "      <td>4.067190</td>\n",
              "      <td>14.040960</td>\n",
              "      <td>15.363394</td>\n",
              "      <td>0.94</td>\n",
              "      <td>1.278596e+00</td>\n",
              "      <td>2.447368</td>\n",
              "      <td>4.622004</td>\n",
              "      <td>3.166859</td>\n",
              "      <td>5.746444</td>\n",
              "      <td>0.107650</td>\n",
              "      <td>1.819269</td>\n",
              "      <td>0.143555</td>\n",
              "      <td>12.384151</td>\n",
              "      <td>4.847826</td>\n",
              "      <td>8.501440</td>\n",
              "      <td>1.471080</td>\n",
              "      <td>3.349110</td>\n",
              "      <td>6.306657</td>\n",
              "      <td>1.007276</td>\n",
              "      <td>0.134101</td>\n",
              "      <td>4.966871</td>\n",
              "      <td>11.419689</td>\n",
              "      <td>7.254098</td>\n",
              "      <td>9.757191</td>\n",
              "      <td>8.482101</td>\n",
              "      <td>5.228867</td>\n",
              "      <td>0.046546</td>\n",
              "      <td>8.656773</td>\n",
              "      <td>...</td>\n",
              "      <td>13.779983</td>\n",
              "      <td>3.307613</td>\n",
              "      <td>2.003458</td>\n",
              "      <td>14.297207</td>\n",
              "      <td>8.174351</td>\n",
              "      <td>2.670522</td>\n",
              "      <td>0.042879</td>\n",
              "      <td>0.739193</td>\n",
              "      <td>0.419732</td>\n",
              "      <td>2.831101</td>\n",
              "      <td>0.219472</td>\n",
              "      <td>15.418033</td>\n",
              "      <td>3.528015</td>\n",
              "      <td>0.482420</td>\n",
              "      <td>0.934582</td>\n",
              "      <td>6.775936</td>\n",
              "      <td>3.052738</td>\n",
              "      <td>9.836066</td>\n",
              "      <td>9.746183</td>\n",
              "      <td>8.097982</td>\n",
              "      <td>17.479207</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.027439</td>\n",
              "      <td>0.246158</td>\n",
              "      <td>8.189655</td>\n",
              "      <td>7.226496e-01</td>\n",
              "      <td>7.237598</td>\n",
              "      <td>0.643228</td>\n",
              "      <td>1.168033</td>\n",
              "      <td>1.222773</td>\n",
              "      <td>1.340944</td>\n",
              "      <td>7.240058</td>\n",
              "      <td>2.417235</td>\n",
              "      <td>15.194609</td>\n",
              "      <td>13.553772</td>\n",
              "      <td>7.229971</td>\n",
              "      <td>0.835158</td>\n",
              "      <td>2.234482</td>\n",
              "      <td>0.946170</td>\n",
              "      <td>2.700606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.909506</td>\n",
              "      <td>10.576516</td>\n",
              "      <td>0.934191</td>\n",
              "      <td>3.419572</td>\n",
              "      <td>1.871438</td>\n",
              "      <td>3.340811</td>\n",
              "      <td>1.868282</td>\n",
              "      <td>0.439865</td>\n",
              "      <td>13.585620</td>\n",
              "      <td>1.153366</td>\n",
              "      <td>9.297974</td>\n",
              "      <td>1.682118</td>\n",
              "      <td>9.553305</td>\n",
              "      <td>10.341188</td>\n",
              "      <td>9.436362</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.548740e+01</td>\n",
              "      <td>1.888375</td>\n",
              "      <td>5.975678</td>\n",
              "      <td>1.775326</td>\n",
              "      <td>9.281851</td>\n",
              "      <td>1.350273</td>\n",
              "      <td>3.208565</td>\n",
              "      <td>1.935790</td>\n",
              "      <td>13.324833</td>\n",
              "      <td>1.725549</td>\n",
              "      <td>8.584763</td>\n",
              "      <td>1.643524</td>\n",
              "      <td>4.157284</td>\n",
              "      <td>6.604193</td>\n",
              "      <td>0.677463</td>\n",
              "      <td>1.667245</td>\n",
              "      <td>8.294594</td>\n",
              "      <td>11.017030</td>\n",
              "      <td>5.779013</td>\n",
              "      <td>10.643856</td>\n",
              "      <td>3.344048</td>\n",
              "      <td>4.260534</td>\n",
              "      <td>1.654864</td>\n",
              "      <td>9.104239</td>\n",
              "      <td>...</td>\n",
              "      <td>16.509023</td>\n",
              "      <td>3.350297</td>\n",
              "      <td>1.434873</td>\n",
              "      <td>13.899021</td>\n",
              "      <td>6.759006</td>\n",
              "      <td>3.237689</td>\n",
              "      <td>1.895391</td>\n",
              "      <td>1.314089</td>\n",
              "      <td>0.859594</td>\n",
              "      <td>6.241737</td>\n",
              "      <td>15.391528</td>\n",
              "      <td>13.914507</td>\n",
              "      <td>3.217597</td>\n",
              "      <td>1.844947</td>\n",
              "      <td>3.843864</td>\n",
              "      <td>5.504495</td>\n",
              "      <td>0.623270</td>\n",
              "      <td>7.723457</td>\n",
              "      <td>6.303451</td>\n",
              "      <td>7.755435</td>\n",
              "      <td>16.618457</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.022681</td>\n",
              "      <td>0.312128</td>\n",
              "      <td>7.819771</td>\n",
              "      <td>6.676273e-07</td>\n",
              "      <td>5.777892</td>\n",
              "      <td>2.743704</td>\n",
              "      <td>2.700285</td>\n",
              "      <td>1.897730</td>\n",
              "      <td>2.738095</td>\n",
              "      <td>6.565509</td>\n",
              "      <td>4.341414</td>\n",
              "      <td>15.893832</td>\n",
              "      <td>11.929787</td>\n",
              "      <td>6.954033</td>\n",
              "      <td>1.853364</td>\n",
              "      <td>0.511027</td>\n",
              "      <td>2.599562</td>\n",
              "      <td>0.811364</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 109 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1f3f79a-a47c-4000-8d61-b5aa29f385fd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c1f3f79a-a47c-4000-8d61-b5aa29f385fd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c1f3f79a-a47c-4000-8d61-b5aa29f385fd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      var_1      var_2      var_3  ...   var_107   var_108   var_109\n",
              "0  4.532710   3.280834  17.982476  ...  1.973832  0.976806  2.541417\n",
              "1  5.821374  12.098722  13.309151  ...  0.567939  1.018818  1.416433\n",
              "2  1.938776   7.952752   0.972671  ...  1.959183  1.575493  1.857893\n",
              "3  6.020690   9.900544  17.869637  ...  2.234482  0.946170  2.700606\n",
              "4  3.909506  10.576516   0.934191  ...  0.511027  2.599562  0.811364\n",
              "\n",
              "[5 rows x 109 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jah7vvgq4HTw"
      },
      "source": [
        "**Important**\n",
        "\n",
        "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "05tv48Wh4HTw",
        "outputId": "f60b542a-4821-420b-af5f-5c4a6c905854",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((35000, 108), (15000, 108))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# separate train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop(labels=['target'], axis=1),\n",
        "    data['target'],\n",
        "    test_size=0.3,\n",
        "    random_state=0)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-5x2j0D4HTx"
      },
      "source": [
        "### Remove correlated features\n",
        "\n",
        "Step Backward Feature Selection takes a long time to run, so to speed it up we will reduce the feature space by removing correlated features first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fBcC0O5I4HTx",
        "outputId": "3d3d77bb-6c76-4989-9510-f03c3afa46c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correlated features:  36\n"
          ]
        }
      ],
      "source": [
        "# remove correlated features to reduce the feature space\n",
        "\n",
        "def correlation(dataset, threshold):\n",
        "    col_corr = set()  # Set of all the names of correlated columns\n",
        "    corr_matrix = dataset.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
        "                colname = corr_matrix.columns[i]  # getting the name of column\n",
        "                col_corr.add(colname)\n",
        "    return col_corr\n",
        "\n",
        "corr_features = correlation(X_train, 0.8)\n",
        "print('correlated features: ', len(set(corr_features)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "z2ezbj7m4HTy",
        "outputId": "45e181b6-f8af-41a6-c2ff-4d4b8d9970bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((35000, 72), (15000, 72))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# removed correlated  features\n",
        "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
        "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArzkBJ-34HTy"
      },
      "source": [
        "### Step Backward Feature Selection\n",
        "\n",
        "For the Step Backward feature selection algorithm, we are going to use the class SFS from MLXtend:\n",
        "http://rasbt.github.io/mlxtend/user_guide/feature_selection/SequentialFeatureSelector/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TrChEoIo4HTy",
        "outputId": "d26ec393-5833-4cbd-b3a9-b2545577ba43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed:  4.1min finished\n",
            "\n",
            "[2022-01-18 11:15:30] Features: 71/65 -- score: 0.6268872842282114[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  71 out of  71 | elapsed:  4.0min finished\n",
            "\n",
            "[2022-01-18 11:19:33] Features: 70/65 -- score: 0.6307351898704118[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed:  4.0min finished\n",
            "\n",
            "[2022-01-18 11:23:34] Features: 69/65 -- score: 0.6300809921574941[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  69 out of  69 | elapsed:  3.9min finished\n",
            "\n",
            "[2022-01-18 11:27:31] Features: 68/65 -- score: 0.6326173155997817[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  68 out of  68 | elapsed:  3.9min finished\n",
            "\n",
            "[2022-01-18 11:31:23] Features: 67/65 -- score: 0.632735049392825[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.5s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  67 out of  67 | elapsed:  3.8min finished\n",
            "\n",
            "[2022-01-18 11:35:11] Features: 66/65 -- score: 0.6306512985168471[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.2s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  66 out of  66 | elapsed:  3.7min finished\n",
            "\n",
            "[2022-01-18 11:38:51] Features: 65/65 -- score: 0.6303118281824431"
          ]
        }
      ],
      "source": [
        "# within the SFS we indicate:\n",
        "\n",
        "# 1) the algorithm we want to create, in this case RandomForests\n",
        "# (note that I use few trees to speed things up)\n",
        "\n",
        "# 2) the stopping criteria: want to select 50 features\n",
        "\n",
        "# 3) wheter to perform step forward or step backward\n",
        "\n",
        "# 4) the evaluation metric: in this case the roc_auc\n",
        "# 5) the want cross-validation\n",
        "\n",
        "# this is going to take a while, do not despair\n",
        "\n",
        "sfs = SFS(RandomForestClassifier(n_estimators=10, n_jobs=4, random_state=0),\n",
        "          k_features=65, # the lower the features we want, the longer this will take\n",
        "          forward=False,\n",
        "          floating=False,\n",
        "          verbose=2,\n",
        "          scoring='roc_auc',\n",
        "          cv=2)\n",
        "\n",
        "sfs = sfs.fit(np.array(X_train), y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x22JyNXR4HTy"
      },
      "source": [
        "In the previous log, we see that the performance does not decrease, so we could continue removing features. If you have time and patience, why don't you try that?\n",
        "\n",
        "### Compare performance of feature subsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rZDdNaOK4HTz"
      },
      "outputs": [],
      "source": [
        "# function to train random forests and evaluate the performance\n",
        "\n",
        "def run_randomForests(X_train, X_test, y_train, y_test):\n",
        "    rf = RandomForestClassifier(n_estimators=200, random_state=39, max_depth=4)\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    print('Train set')\n",
        "    pred = rf.predict_proba(X_train)\n",
        "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_train, pred[:,1])))\n",
        "    \n",
        "    print('Test set')\n",
        "    pred = rf.predict_proba(X_test)\n",
        "    print('Random Forests roc-auc: {}'.format(roc_auc_score(y_test, pred[:,1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NSWMz2cb4HTz",
        "outputId": "fff3b7cf-f61b-43c7-f6d1-e42aa5258bbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'var_6', 'var_7', 'var_8',\n",
              "       'var_10', 'var_11', 'var_12', 'var_13', 'var_14', 'var_15', 'var_16',\n",
              "       'var_18', 'var_19', 'var_20', 'var_21', 'var_22', 'var_23', 'var_25',\n",
              "       'var_26', 'var_27', 'var_30', 'var_31', 'var_34', 'var_35', 'var_37',\n",
              "       'var_40', 'var_41', 'var_45', 'var_47', 'var_48', 'var_49', 'var_50',\n",
              "       'var_51', 'var_52', 'var_53', 'var_55', 'var_56', 'var_58', 'var_60',\n",
              "       'var_62', 'var_63', 'var_67', 'var_68', 'var_69', 'var_71', 'var_73',\n",
              "       'var_77', 'var_78', 'var_79', 'var_81', 'var_82', 'var_83', 'var_86',\n",
              "       'var_89', 'var_90', 'var_91', 'var_93', 'var_96', 'var_98', 'var_103',\n",
              "       'var_107'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "selected_feat= X_train.columns[list(sfs.k_feature_idx_)]\n",
        "\n",
        "selected_feat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RqP_9NwU4HTz",
        "outputId": "25cb7d98-166f-4a96-b218-a08a0560ef76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set\n",
            "Random Forests roc-auc: 0.7118554905228884\n",
            "Test set\n",
            "Random Forests roc-auc: 0.6960298383148206\n"
          ]
        }
      ],
      "source": [
        "# evaluate performance of algorithm built\n",
        "# using selected features\n",
        "\n",
        "run_randomForests(X_train[selected_feat],\n",
        "                  X_test[selected_feat],\n",
        "                  y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "z7hiTrQL4HTz",
        "outputId": "ed601e7f-3080-4dac-e01a-7abdff8a6d41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set\n",
            "Random Forests roc-auc: 0.7119921185820277\n",
            "Test set\n",
            "Random Forests roc-auc: 0.6957598691250635\n"
          ]
        }
      ],
      "source": [
        "# and for comparison, we train random forests using\n",
        "# all features (except the correlated ones, which we removed already)\n",
        "\n",
        "run_randomForests(X_train,\n",
        "                  X_test,\n",
        "                  y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJFvelxk4HT0"
      },
      "source": [
        "Performance, as expected is roughly the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8bwx8Yz4HT0"
      },
      "source": [
        "## Regression\n",
        "\n",
        "Let's now repeat the process but in the context of regression. With the house prices dataset from Kaggle, the aim is to predict the continuous target: House Price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1dPo6SZw4HT0",
        "outputId": "eaf3cc3d-4155-46ab-c450-fcbd1bbedd2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1460, 81)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# load dataset\n",
        "\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/Venkatpandey/DataScience_ML/main/dataset/houseprice.csv')\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Oa6nohjJ4HT0",
        "outputId": "a0f22f98-fb09-45bd-b6e6-73a8c33e7f0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1460, 38)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# In practice, feature selection should be done after data pre-processing,\n",
        "# so ideally, all the categorical variables are encoded into numbers,\n",
        "# and then you can assess how deterministic they are of the target\n",
        "\n",
        "# here for simplicity I will use only numerical variables\n",
        "# select numerical columns:\n",
        "\n",
        "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
        "data = data[numerical_vars]\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IXFDYjdi4HT0",
        "outputId": "83700fcf-7697-4587-e75b-4b77d6863194",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1022, 37), (438, 37))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# separate train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop(labels=['SalePrice'], axis=1),\n",
        "    data['SalePrice'],\n",
        "    test_size=0.3,\n",
        "    random_state=0)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVD5pmd84HT0"
      },
      "source": [
        "### Remove correlated features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oNpCpohw4HT0",
        "outputId": "2d79307f-dee6-4c26-e817-fa7cbef2ee29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correlated features:  3\n"
          ]
        }
      ],
      "source": [
        "# find and remove correlated features\n",
        "\n",
        "def correlation(dataset, threshold):\n",
        "    col_corr = set()  # Set of all the names of correlated columns\n",
        "    corr_matrix = dataset.corr()\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i):\n",
        "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
        "                colname = corr_matrix.columns[i]  # getting the name of column\n",
        "                col_corr.add(colname)\n",
        "    return col_corr\n",
        "\n",
        "corr_features = correlation(X_train, 0.8)\n",
        "print('correlated features: ', len(set(corr_features)) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dN8NXT-s4HT1",
        "outputId": "313c527d-2c8d-4a52-d410-185a7a047d17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1022, 34), (438, 34))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# removed correlated features\n",
        "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
        "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "doTunHk74HT1"
      },
      "outputs": [],
      "source": [
        "X_train.fillna(0, inplace=True)\n",
        "X_test.fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLyqx8a_4HT1"
      },
      "source": [
        "### Step Backward Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "scrolled": true,
        "id": "m8GxH8cX4HT1",
        "outputId": "f71fb67b-a134-49ac-8780-dd3c8c1163f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:   12.0s finished\n",
            "\n",
            "[2022-01-18 11:39:49] Features: 33/20 -- score: 0.825434533342885[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:   11.5s finished\n",
            "\n",
            "[2022-01-18 11:40:01] Features: 32/20 -- score: 0.8269182238540728[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:   11.1s finished\n",
            "\n",
            "[2022-01-18 11:40:12] Features: 31/20 -- score: 0.8321203993856869[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:   10.7s finished\n",
            "\n",
            "[2022-01-18 11:40:23] Features: 30/20 -- score: 0.826565096160356[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   10.3s finished\n",
            "\n",
            "[2022-01-18 11:40:33] Features: 29/20 -- score: 0.8297349081341103[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:    9.8s finished\n",
            "\n",
            "[2022-01-18 11:40:43] Features: 28/20 -- score: 0.8347342098816987[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:    9.4s finished\n",
            "\n",
            "[2022-01-18 11:40:52] Features: 27/20 -- score: 0.8330866197775185[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    9.1s finished\n",
            "\n",
            "[2022-01-18 11:41:01] Features: 26/20 -- score: 0.8384069806635874[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:    8.7s finished\n",
            "\n",
            "[2022-01-18 11:41:10] Features: 25/20 -- score: 0.8332772032724493[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    8.3s finished\n",
            "\n",
            "[2022-01-18 11:41:18] Features: 24/20 -- score: 0.8369514759762875[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    8.0s finished\n",
            "\n",
            "[2022-01-18 11:41:26] Features: 23/20 -- score: 0.8445883592176333[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:    7.6s finished\n",
            "\n",
            "[2022-01-18 11:41:34] Features: 22/20 -- score: 0.8366958736874247[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    7.2s finished\n",
            "\n",
            "[2022-01-18 11:41:41] Features: 21/20 -- score: 0.8420599404686491[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:    6.9s finished\n",
            "\n",
            "[2022-01-18 11:41:48] Features: 20/20 -- score: 0.8384558070101512"
          ]
        }
      ],
      "source": [
        "# step backward feature selection algorithm\n",
        "\n",
        "sfs = SFS(RandomForestRegressor(n_estimators=10, n_jobs=4, random_state=10), \n",
        "           k_features=20, \n",
        "           forward=False, \n",
        "           floating=False, \n",
        "           verbose=2,\n",
        "           scoring='r2',\n",
        "           cv=2)\n",
        "\n",
        "sfs = sfs.fit(np.array(X_train), y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Xop-4xry4HT1",
        "outputId": "9c3be10b-9d02-4cf6-9af3-80bd5340092c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 3, 4, 5, 6, 7, 9, 12, 14, 16, 18, 20, 22, 23, 24, 26, 27, 28, 31, 32)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "sfs.k_feature_idx_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9lbVotCG4HT1",
        "outputId": "1a206e2b-a73a-4cb7-d1a9-117d5d8caf03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Id', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt',\n",
              "       'YearRemodAdd', 'BsmtFinSF1', 'TotalBsmtSF', '2ndFlrSF', 'GrLivArea',\n",
              "       'BsmtHalfBath', 'HalfBath', 'KitchenAbvGr', 'Fireplaces', 'GarageCars',\n",
              "       'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'MiscVal', 'MoSold'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "X_train.columns[list(sfs.k_feature_idx_)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYjPq0h_4HT1"
      },
      "source": [
        "### Compare performance of feature subsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vS1Vqufo4HT1"
      },
      "outputs": [],
      "source": [
        "# function to train random forests and evaluate the performance\n",
        "\n",
        "def run_randomForests(X_train, X_test, y_train, y_test):\n",
        "    \n",
        "    rf = RandomForestRegressor(n_estimators=200, random_state=39, max_depth=4)\n",
        "    rf.fit(X_train, y_train)\n",
        "\n",
        "    print('Train set')\n",
        "    pred = rf.predict(X_train)\n",
        "    print('Random Forests roc-auc: {}'.format(r2_score(y_train, pred)))\n",
        "    \n",
        "    print('Test set')\n",
        "    pred = rf.predict(X_test)\n",
        "    print('Random Forests roc-auc: {}'.format(r2_score(y_test, pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UUQC8DaF4HT1"
      },
      "outputs": [],
      "source": [
        "selected_feat = X_train.columns[list(sfs.k_feature_idx_)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "W47cNECQ4HT1",
        "outputId": "463b253c-2559-4a08-f20e-19c8ec05b656",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set\n",
            "Random Forests roc-auc: 0.8702345974928545\n",
            "Test set\n",
            "Random Forests roc-auc: 0.8240294353877188\n"
          ]
        }
      ],
      "source": [
        "# evaluate performance of algorithm built\n",
        "# using selected features\n",
        "\n",
        "run_randomForests(X_train[selected_feat],\n",
        "                  X_test[selected_feat],\n",
        "                  y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iMaKmaIl4HT1",
        "outputId": "60883540-e8b7-4a93-a2a4-3c6ec568a1cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set\n",
            "Random Forests roc-auc: 0.8699152317492538\n",
            "Test set\n",
            "Random Forests roc-auc: 0.8190809813112794\n"
          ]
        }
      ],
      "source": [
        "# and for comparison, we train random forests using\n",
        "# all features (except the correlated ones, which we removed already)\n",
        "\n",
        "run_randomForests(X_train,\n",
        "                  X_test,\n",
        "                  y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "7ihrMeJT4HT1"
      },
      "source": [
        "This is all for this lecture. I hope you enjoyed it, and see you in the next one!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fsml",
      "language": "python",
      "name": "fsml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": true
    },
    "colab": {
      "name": "07.2-Step-backward-feature-selection.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}