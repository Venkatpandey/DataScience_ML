{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Venkatpandey/DataScience_ML/blob/main/featureSelection/08.1-Logistic-regression-coefficients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKgUtu_j-EBj"
      },
      "source": [
        "## Logistic Regression Coefficients\n",
        "\n",
        "Linear regression predicts a quantitative response Y on the basis of predictor variables X1, X2, ... Xn. It assumes that there is a linear relationship between X(s) and Y. Mathematically, we write this linear relationship as Y ≈ β0 + β1X1 + β2X2 + ... + βnXn.\n",
        "\n",
        "**The magnitude of the coefficients is directly influenced by the scale of the features**. Therefore, to compare coefficients across features, it is importance that all features are within a similar scale. This is why, normalisation is important for variable importance and feature selection in linear models.\n",
        "\n",
        "Linear Regression makes the following assumptions over the predictor variables X:\n",
        "- Linear relationship with the outcome Y\n",
        "- Multivariate normality (X should follow a Gaussian distribution)\n",
        "- No or little multicollinearity (Xs should not be linearly related to one another)\n",
        "- Homoscedasticity (variance should be the same)\n",
        "\n",
        "Homoscedasticity, also known as homogeneity of variance, describes a situation in which the error term (that is, the “noise” or random disturbance in the relationship between the independent variables (Xs) and the dependent variable (Y)) is the same across all values of the independent variables.\n",
        "\n",
        "Therefore, there are a lot of assumptions that need to be met in order to make a fair comparison of the features by using only their regression coefficients.\n",
        "\n",
        "In addition, these coefficients may be penalised by regularisation, therefore being smaller than if we were to compare the relationship of each feature with the target individually.\n",
        "\n",
        "Having said this, you can still select features based on linear regression coefficients, provided you keep all of these in mind at the time of analysing the outcome.\n",
        "\n",
        "Personally, this is not my feature selection method of choice, although I find it useful to interpret the output of the model.\n",
        "\n",
        "I will demonstrate how to select features based in a regression and a classification scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vxyoMEme-EBl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SelectFromModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rYxD1ywB-EBm",
        "outputId": "d03e728d-cc99-4e34-89a2-00130bca9fd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 109)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# load dataset\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/Venkatpandey/DataScience_ML/main/dataset/dataset_2.csv')\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0nTeL7Ch-EBn",
        "outputId": "5ec9e450-9491-44a2-ec82-65604c70fed7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f8772cab-9649-4755-aa54-721b0baee469\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>var_1</th>\n",
              "      <th>var_2</th>\n",
              "      <th>var_3</th>\n",
              "      <th>var_4</th>\n",
              "      <th>var_5</th>\n",
              "      <th>var_6</th>\n",
              "      <th>var_7</th>\n",
              "      <th>var_8</th>\n",
              "      <th>var_9</th>\n",
              "      <th>var_10</th>\n",
              "      <th>var_11</th>\n",
              "      <th>var_12</th>\n",
              "      <th>var_13</th>\n",
              "      <th>var_14</th>\n",
              "      <th>var_15</th>\n",
              "      <th>var_16</th>\n",
              "      <th>var_17</th>\n",
              "      <th>var_18</th>\n",
              "      <th>var_19</th>\n",
              "      <th>var_20</th>\n",
              "      <th>var_21</th>\n",
              "      <th>var_22</th>\n",
              "      <th>var_23</th>\n",
              "      <th>var_24</th>\n",
              "      <th>var_25</th>\n",
              "      <th>var_26</th>\n",
              "      <th>var_27</th>\n",
              "      <th>var_28</th>\n",
              "      <th>var_29</th>\n",
              "      <th>var_30</th>\n",
              "      <th>var_31</th>\n",
              "      <th>var_32</th>\n",
              "      <th>var_33</th>\n",
              "      <th>var_34</th>\n",
              "      <th>var_35</th>\n",
              "      <th>var_36</th>\n",
              "      <th>var_37</th>\n",
              "      <th>var_38</th>\n",
              "      <th>var_39</th>\n",
              "      <th>var_40</th>\n",
              "      <th>...</th>\n",
              "      <th>var_70</th>\n",
              "      <th>var_71</th>\n",
              "      <th>var_72</th>\n",
              "      <th>var_73</th>\n",
              "      <th>var_74</th>\n",
              "      <th>var_75</th>\n",
              "      <th>var_76</th>\n",
              "      <th>var_77</th>\n",
              "      <th>var_78</th>\n",
              "      <th>var_79</th>\n",
              "      <th>var_80</th>\n",
              "      <th>var_81</th>\n",
              "      <th>var_82</th>\n",
              "      <th>var_83</th>\n",
              "      <th>var_84</th>\n",
              "      <th>var_85</th>\n",
              "      <th>var_86</th>\n",
              "      <th>var_87</th>\n",
              "      <th>var_88</th>\n",
              "      <th>var_89</th>\n",
              "      <th>var_90</th>\n",
              "      <th>var_91</th>\n",
              "      <th>var_92</th>\n",
              "      <th>var_93</th>\n",
              "      <th>var_94</th>\n",
              "      <th>var_95</th>\n",
              "      <th>var_96</th>\n",
              "      <th>var_97</th>\n",
              "      <th>var_98</th>\n",
              "      <th>var_99</th>\n",
              "      <th>var_100</th>\n",
              "      <th>var_101</th>\n",
              "      <th>var_102</th>\n",
              "      <th>var_103</th>\n",
              "      <th>var_104</th>\n",
              "      <th>var_105</th>\n",
              "      <th>var_106</th>\n",
              "      <th>var_107</th>\n",
              "      <th>var_108</th>\n",
              "      <th>var_109</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.532710</td>\n",
              "      <td>3.280834</td>\n",
              "      <td>17.982476</td>\n",
              "      <td>4.404259</td>\n",
              "      <td>2.349910</td>\n",
              "      <td>0.603264</td>\n",
              "      <td>2.784655</td>\n",
              "      <td>0.323146</td>\n",
              "      <td>12.009691</td>\n",
              "      <td>0.139346</td>\n",
              "      <td>5.751633</td>\n",
              "      <td>2.808895</td>\n",
              "      <td>1.244055</td>\n",
              "      <td>11.269688</td>\n",
              "      <td>15.866550</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.181500e+00</td>\n",
              "      <td>1.903910</td>\n",
              "      <td>4.667888</td>\n",
              "      <td>1.842749</td>\n",
              "      <td>5.863767</td>\n",
              "      <td>0.115498</td>\n",
              "      <td>2.398785</td>\n",
              "      <td>0.139191</td>\n",
              "      <td>11.860244</td>\n",
              "      <td>4.433561</td>\n",
              "      <td>7.135750</td>\n",
              "      <td>2.240605</td>\n",
              "      <td>3.720161</td>\n",
              "      <td>5.805012</td>\n",
              "      <td>1.308222</td>\n",
              "      <td>0.133272</td>\n",
              "      <td>5.514540</td>\n",
              "      <td>11.510708</td>\n",
              "      <td>7.534482</td>\n",
              "      <td>8.779925</td>\n",
              "      <td>6.797556</td>\n",
              "      <td>8.504757</td>\n",
              "      <td>0.188741</td>\n",
              "      <td>8.783980</td>\n",
              "      <td>...</td>\n",
              "      <td>12.866988</td>\n",
              "      <td>11.369994</td>\n",
              "      <td>1.467595</td>\n",
              "      <td>10.043070</td>\n",
              "      <td>8.174325</td>\n",
              "      <td>2.088815</td>\n",
              "      <td>0.134455</td>\n",
              "      <td>1.282842</td>\n",
              "      <td>1.262513</td>\n",
              "      <td>1.114369</td>\n",
              "      <td>1.446358</td>\n",
              "      <td>15.512397</td>\n",
              "      <td>1.820403</td>\n",
              "      <td>0.619730</td>\n",
              "      <td>0.826138</td>\n",
              "      <td>6.880270</td>\n",
              "      <td>1.680353</td>\n",
              "      <td>8.659387</td>\n",
              "      <td>10.184313</td>\n",
              "      <td>7.248146</td>\n",
              "      <td>17.065003</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.044600</td>\n",
              "      <td>0.176036</td>\n",
              "      <td>9.869159</td>\n",
              "      <td>4.662407e-01</td>\n",
              "      <td>7.273476</td>\n",
              "      <td>0.623398</td>\n",
              "      <td>2.070677</td>\n",
              "      <td>1.108609</td>\n",
              "      <td>2.079066</td>\n",
              "      <td>6.748819</td>\n",
              "      <td>2.941445</td>\n",
              "      <td>18.360496</td>\n",
              "      <td>17.726613</td>\n",
              "      <td>7.774031</td>\n",
              "      <td>1.473441</td>\n",
              "      <td>1.973832</td>\n",
              "      <td>0.976806</td>\n",
              "      <td>2.541417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.821374</td>\n",
              "      <td>12.098722</td>\n",
              "      <td>13.309151</td>\n",
              "      <td>4.125599</td>\n",
              "      <td>1.045386</td>\n",
              "      <td>1.832035</td>\n",
              "      <td>1.833494</td>\n",
              "      <td>0.709090</td>\n",
              "      <td>8.652883</td>\n",
              "      <td>0.102757</td>\n",
              "      <td>8.225109</td>\n",
              "      <td>2.001220</td>\n",
              "      <td>8.081647</td>\n",
              "      <td>3.933986</td>\n",
              "      <td>14.350374</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.244384e+01</td>\n",
              "      <td>1.575456</td>\n",
              "      <td>5.275010</td>\n",
              "      <td>2.750981</td>\n",
              "      <td>3.402345</td>\n",
              "      <td>0.227527</td>\n",
              "      <td>2.502344</td>\n",
              "      <td>0.197449</td>\n",
              "      <td>12.654514</td>\n",
              "      <td>3.895271</td>\n",
              "      <td>9.230702</td>\n",
              "      <td>0.719196</td>\n",
              "      <td>3.393035</td>\n",
              "      <td>6.055243</td>\n",
              "      <td>0.926661</td>\n",
              "      <td>0.221227</td>\n",
              "      <td>7.406060</td>\n",
              "      <td>10.290955</td>\n",
              "      <td>8.075000</td>\n",
              "      <td>10.034637</td>\n",
              "      <td>6.182029</td>\n",
              "      <td>7.698029</td>\n",
              "      <td>0.295115</td>\n",
              "      <td>10.308592</td>\n",
              "      <td>...</td>\n",
              "      <td>10.477765</td>\n",
              "      <td>3.026453</td>\n",
              "      <td>1.338741</td>\n",
              "      <td>16.136215</td>\n",
              "      <td>8.659485</td>\n",
              "      <td>0.567717</td>\n",
              "      <td>0.108499</td>\n",
              "      <td>1.447928</td>\n",
              "      <td>0.583342</td>\n",
              "      <td>4.454525</td>\n",
              "      <td>3.570452</td>\n",
              "      <td>15.988817</td>\n",
              "      <td>2.628892</td>\n",
              "      <td>1.251810</td>\n",
              "      <td>2.077105</td>\n",
              "      <td>7.453729</td>\n",
              "      <td>2.173920</td>\n",
              "      <td>10.357143</td>\n",
              "      <td>13.274292</td>\n",
              "      <td>8.647012</td>\n",
              "      <td>17.143991</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.161626</td>\n",
              "      <td>0.214995</td>\n",
              "      <td>8.661069</td>\n",
              "      <td>9.585002e-01</td>\n",
              "      <td>6.475936</td>\n",
              "      <td>1.230876</td>\n",
              "      <td>2.249656</td>\n",
              "      <td>0.615216</td>\n",
              "      <td>2.479789</td>\n",
              "      <td>7.795290</td>\n",
              "      <td>3.557890</td>\n",
              "      <td>17.383378</td>\n",
              "      <td>15.193423</td>\n",
              "      <td>8.263673</td>\n",
              "      <td>1.878108</td>\n",
              "      <td>0.567939</td>\n",
              "      <td>1.018818</td>\n",
              "      <td>1.416433</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.938776</td>\n",
              "      <td>7.952752</td>\n",
              "      <td>0.972671</td>\n",
              "      <td>3.459267</td>\n",
              "      <td>1.935782</td>\n",
              "      <td>0.621463</td>\n",
              "      <td>2.338139</td>\n",
              "      <td>0.344948</td>\n",
              "      <td>9.937850</td>\n",
              "      <td>11.691283</td>\n",
              "      <td>8.307318</td>\n",
              "      <td>3.239122</td>\n",
              "      <td>2.699376</td>\n",
              "      <td>10.030416</td>\n",
              "      <td>14.977220</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.636780e-07</td>\n",
              "      <td>2.605838</td>\n",
              "      <td>5.459521</td>\n",
              "      <td>3.437779</td>\n",
              "      <td>5.498281</td>\n",
              "      <td>19.800000</td>\n",
              "      <td>2.136717</td>\n",
              "      <td>19.036815</td>\n",
              "      <td>11.938497</td>\n",
              "      <td>4.378310</td>\n",
              "      <td>6.843868</td>\n",
              "      <td>1.745698</td>\n",
              "      <td>3.721307</td>\n",
              "      <td>6.339151</td>\n",
              "      <td>1.479797</td>\n",
              "      <td>18.600001</td>\n",
              "      <td>8.142160</td>\n",
              "      <td>12.575593</td>\n",
              "      <td>6.752941</td>\n",
              "      <td>6.303391</td>\n",
              "      <td>5.327748</td>\n",
              "      <td>7.559745</td>\n",
              "      <td>16.951823</td>\n",
              "      <td>7.701432</td>\n",
              "      <td>...</td>\n",
              "      <td>12.795940</td>\n",
              "      <td>3.158102</td>\n",
              "      <td>2.084452</td>\n",
              "      <td>13.596735</td>\n",
              "      <td>7.136616</td>\n",
              "      <td>3.975333</td>\n",
              "      <td>19.199999</td>\n",
              "      <td>1.035094</td>\n",
              "      <td>1.039650</td>\n",
              "      <td>2.920388</td>\n",
              "      <td>18.194969</td>\n",
              "      <td>13.878539</td>\n",
              "      <td>4.177674</td>\n",
              "      <td>0.265892</td>\n",
              "      <td>0.949150</td>\n",
              "      <td>5.501881</td>\n",
              "      <td>1.545747</td>\n",
              "      <td>6.652942</td>\n",
              "      <td>10.219311</td>\n",
              "      <td>7.350044</td>\n",
              "      <td>15.865534</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.668244</td>\n",
              "      <td>0.207304</td>\n",
              "      <td>9.591838</td>\n",
              "      <td>1.426163e+00</td>\n",
              "      <td>7.552225</td>\n",
              "      <td>0.599195</td>\n",
              "      <td>1.872145</td>\n",
              "      <td>2.111624</td>\n",
              "      <td>1.861487</td>\n",
              "      <td>6.130886</td>\n",
              "      <td>3.401064</td>\n",
              "      <td>15.850471</td>\n",
              "      <td>14.620599</td>\n",
              "      <td>6.849776</td>\n",
              "      <td>1.098210</td>\n",
              "      <td>1.959183</td>\n",
              "      <td>1.575493</td>\n",
              "      <td>1.857893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.020690</td>\n",
              "      <td>9.900544</td>\n",
              "      <td>17.869637</td>\n",
              "      <td>4.366715</td>\n",
              "      <td>1.973693</td>\n",
              "      <td>2.026012</td>\n",
              "      <td>2.853025</td>\n",
              "      <td>0.674847</td>\n",
              "      <td>11.816859</td>\n",
              "      <td>0.011151</td>\n",
              "      <td>5.769939</td>\n",
              "      <td>2.760518</td>\n",
              "      <td>4.067190</td>\n",
              "      <td>14.040960</td>\n",
              "      <td>15.363394</td>\n",
              "      <td>0.94</td>\n",
              "      <td>1.278596e+00</td>\n",
              "      <td>2.447368</td>\n",
              "      <td>4.622004</td>\n",
              "      <td>3.166859</td>\n",
              "      <td>5.746444</td>\n",
              "      <td>0.107650</td>\n",
              "      <td>1.819269</td>\n",
              "      <td>0.143555</td>\n",
              "      <td>12.384151</td>\n",
              "      <td>4.847826</td>\n",
              "      <td>8.501440</td>\n",
              "      <td>1.471080</td>\n",
              "      <td>3.349110</td>\n",
              "      <td>6.306657</td>\n",
              "      <td>1.007276</td>\n",
              "      <td>0.134101</td>\n",
              "      <td>4.966871</td>\n",
              "      <td>11.419689</td>\n",
              "      <td>7.254098</td>\n",
              "      <td>9.757191</td>\n",
              "      <td>8.482101</td>\n",
              "      <td>5.228867</td>\n",
              "      <td>0.046546</td>\n",
              "      <td>8.656773</td>\n",
              "      <td>...</td>\n",
              "      <td>13.779983</td>\n",
              "      <td>3.307613</td>\n",
              "      <td>2.003458</td>\n",
              "      <td>14.297207</td>\n",
              "      <td>8.174351</td>\n",
              "      <td>2.670522</td>\n",
              "      <td>0.042879</td>\n",
              "      <td>0.739193</td>\n",
              "      <td>0.419732</td>\n",
              "      <td>2.831101</td>\n",
              "      <td>0.219472</td>\n",
              "      <td>15.418033</td>\n",
              "      <td>3.528015</td>\n",
              "      <td>0.482420</td>\n",
              "      <td>0.934582</td>\n",
              "      <td>6.775936</td>\n",
              "      <td>3.052738</td>\n",
              "      <td>9.836066</td>\n",
              "      <td>9.746183</td>\n",
              "      <td>8.097982</td>\n",
              "      <td>17.479207</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.027439</td>\n",
              "      <td>0.246158</td>\n",
              "      <td>8.189655</td>\n",
              "      <td>7.226496e-01</td>\n",
              "      <td>7.237598</td>\n",
              "      <td>0.643228</td>\n",
              "      <td>1.168033</td>\n",
              "      <td>1.222773</td>\n",
              "      <td>1.340944</td>\n",
              "      <td>7.240058</td>\n",
              "      <td>2.417235</td>\n",
              "      <td>15.194609</td>\n",
              "      <td>13.553772</td>\n",
              "      <td>7.229971</td>\n",
              "      <td>0.835158</td>\n",
              "      <td>2.234482</td>\n",
              "      <td>0.946170</td>\n",
              "      <td>2.700606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.909506</td>\n",
              "      <td>10.576516</td>\n",
              "      <td>0.934191</td>\n",
              "      <td>3.419572</td>\n",
              "      <td>1.871438</td>\n",
              "      <td>3.340811</td>\n",
              "      <td>1.868282</td>\n",
              "      <td>0.439865</td>\n",
              "      <td>13.585620</td>\n",
              "      <td>1.153366</td>\n",
              "      <td>9.297974</td>\n",
              "      <td>1.682118</td>\n",
              "      <td>9.553305</td>\n",
              "      <td>10.341188</td>\n",
              "      <td>9.436362</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.548740e+01</td>\n",
              "      <td>1.888375</td>\n",
              "      <td>5.975678</td>\n",
              "      <td>1.775326</td>\n",
              "      <td>9.281851</td>\n",
              "      <td>1.350273</td>\n",
              "      <td>3.208565</td>\n",
              "      <td>1.935790</td>\n",
              "      <td>13.324833</td>\n",
              "      <td>1.725549</td>\n",
              "      <td>8.584763</td>\n",
              "      <td>1.643524</td>\n",
              "      <td>4.157284</td>\n",
              "      <td>6.604193</td>\n",
              "      <td>0.677463</td>\n",
              "      <td>1.667245</td>\n",
              "      <td>8.294594</td>\n",
              "      <td>11.017030</td>\n",
              "      <td>5.779013</td>\n",
              "      <td>10.643856</td>\n",
              "      <td>3.344048</td>\n",
              "      <td>4.260534</td>\n",
              "      <td>1.654864</td>\n",
              "      <td>9.104239</td>\n",
              "      <td>...</td>\n",
              "      <td>16.509023</td>\n",
              "      <td>3.350297</td>\n",
              "      <td>1.434873</td>\n",
              "      <td>13.899021</td>\n",
              "      <td>6.759006</td>\n",
              "      <td>3.237689</td>\n",
              "      <td>1.895391</td>\n",
              "      <td>1.314089</td>\n",
              "      <td>0.859594</td>\n",
              "      <td>6.241737</td>\n",
              "      <td>15.391528</td>\n",
              "      <td>13.914507</td>\n",
              "      <td>3.217597</td>\n",
              "      <td>1.844947</td>\n",
              "      <td>3.843864</td>\n",
              "      <td>5.504495</td>\n",
              "      <td>0.623270</td>\n",
              "      <td>7.723457</td>\n",
              "      <td>6.303451</td>\n",
              "      <td>7.755435</td>\n",
              "      <td>16.618457</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.022681</td>\n",
              "      <td>0.312128</td>\n",
              "      <td>7.819771</td>\n",
              "      <td>6.676273e-07</td>\n",
              "      <td>5.777892</td>\n",
              "      <td>2.743704</td>\n",
              "      <td>2.700285</td>\n",
              "      <td>1.897730</td>\n",
              "      <td>2.738095</td>\n",
              "      <td>6.565509</td>\n",
              "      <td>4.341414</td>\n",
              "      <td>15.893832</td>\n",
              "      <td>11.929787</td>\n",
              "      <td>6.954033</td>\n",
              "      <td>1.853364</td>\n",
              "      <td>0.511027</td>\n",
              "      <td>2.599562</td>\n",
              "      <td>0.811364</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 109 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8772cab-9649-4755-aa54-721b0baee469')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f8772cab-9649-4755-aa54-721b0baee469 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f8772cab-9649-4755-aa54-721b0baee469');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      var_1      var_2      var_3  ...   var_107   var_108   var_109\n",
              "0  4.532710   3.280834  17.982476  ...  1.973832  0.976806  2.541417\n",
              "1  5.821374  12.098722  13.309151  ...  0.567939  1.018818  1.416433\n",
              "2  1.938776   7.952752   0.972671  ...  1.959183  1.575493  1.857893\n",
              "3  6.020690   9.900544  17.869637  ...  2.234482  0.946170  2.700606\n",
              "4  3.909506  10.576516   0.934191  ...  0.511027  2.599562  0.811364\n",
              "\n",
              "[5 rows x 109 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojpfMirU-EBn"
      },
      "source": [
        "**Important**\n",
        "\n",
        "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "01Kc461k-EBn",
        "outputId": "80d72345-279d-46c7-ecfe-2e5e44c6af4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((35000, 108), (15000, 108))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# separate train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop(labels=['target'], axis=1),\n",
        "    data['target'],\n",
        "    test_size=0.3,\n",
        "    random_state=0)\n",
        "\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PTEiFcGc-EBo",
        "outputId": "0ef0e32c-44e7-4bc8-f87d-e9faa5601b96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler()"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# we will scale the variables, so we fit a scaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "scrolled": true,
        "id": "ClyvoENH-EBo",
        "outputId": "72124fcd-60ee-43d2-80bf-b5021582be76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SelectFromModel(estimator=LogisticRegression(C=1000, max_iter=300,\n",
              "                                             random_state=10))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# here I will do the model fitting and feature selection\n",
        "# altogether in 2 lines of code\n",
        "\n",
        "# first I specify the Logistic Regression model, here I\n",
        "# select the Ridge Penalty (l2)(it is the default parameter in sklearn)\n",
        "\n",
        "# remember that here I want to evaluate the coefficient magnitud\n",
        "# itself and not whether lasso shrinks coefficients to zero\n",
        "\n",
        "# ideally, I want to avoid regularisation at all, so the coefficients\n",
        "# are not affected (modified) by the penalty of the regularisation\n",
        "\n",
        "# In order to do this in sklearn, I set the parameter C really high\n",
        "# which is basically like fitting a non-regularised logistic regression\n",
        "\n",
        "# Then I use the selectFromModel object from sklearn\n",
        "# to automatically select the features\n",
        "\n",
        "# set C to 1000, to avoid regularisation\n",
        "sel_ = SelectFromModel(\n",
        "    LogisticRegression(C=1000, penalty='l2', max_iter=300, random_state=10))\n",
        "\n",
        "sel_.fit(scaler.transform(X_train), y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "P7ycbh_8-EBp",
        "outputId": "77f3a8be-0619-4370-f984-f1d44154c061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False,  True, False, False, False, False, False, False,\n",
              "       False,  True, False, False, False, False, False, False, False,\n",
              "       False, False,  True, False,  True,  True, False,  True, False,\n",
              "       False, False, False, False,  True,  True, False, False, False,\n",
              "       False, False,  True,  True, False, False, False, False, False,\n",
              "       False, False,  True, False,  True, False,  True, False, False,\n",
              "        True,  True, False, False, False,  True, False,  True, False,\n",
              "       False, False, False, False,  True,  True, False,  True, False,\n",
              "        True, False, False,  True, False, False,  True, False, False,\n",
              "        True,  True, False, False, False,  True,  True, False,  True,\n",
              "       False,  True, False, False, False, False,  True, False,  True,\n",
              "       False,  True, False, False, False,  True, False, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# this command let's me visualise those features that were kept.\n",
        "\n",
        "# sklearn will select those features which coefficients are greater\n",
        "# than the mean of all the coefficients.\n",
        "\n",
        "# it compares absolute values of coefficients. More on this in a second.\n",
        "\n",
        "sel_.get_support()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "scrolled": true,
        "id": "HnHGEtjS-EBp",
        "outputId": "b57feba1-0309-4969-83ca-606923f4535c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# let's add the variable names and order it for clearer visualisation\n",
        "# and then let's sum the number of selected features\n",
        "\n",
        "selected_feat = X_train.columns[(sel_.get_support())]\n",
        "len(selected_feat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "scrolled": true,
        "id": "u_-znR_n-EBq",
        "outputId": "830f6795-bf07-4b0d-9b89-f726b0d1c5e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.15512029e-02, -1.34505010e-02,  1.08760024e-01,\n",
              "         2.64568016e-02,  4.34834833e-02, -2.46493535e-02,\n",
              "        -3.35525894e-02, -6.25472762e-03,  2.32102988e-02,\n",
              "        -9.83651141e-03,  8.01329160e-02,  5.28320521e-02,\n",
              "        -3.75400666e-02,  1.92307372e-02, -3.85761325e-02,\n",
              "         2.67134758e-02,  1.36666777e-02,  5.41664070e-02,\n",
              "         1.29705505e-02,  1.54086056e-02,  1.42006474e-01,\n",
              "         2.47858316e-02, -1.28802530e-01,  1.26653140e-01,\n",
              "         1.43390977e-02,  6.33685754e-02,  2.75230584e-02,\n",
              "        -9.61557260e-04, -1.41870866e-02, -1.45065753e-02,\n",
              "         6.86812255e-03, -1.86755677e-01, -1.00929951e-01,\n",
              "         3.28164228e-02,  1.18712742e-02,  4.34298476e-02,\n",
              "         1.96184212e-02,  4.49277707e-02,  7.94011232e-02,\n",
              "         6.99691608e-02, -3.53926130e-03, -2.07548934e-03,\n",
              "        -5.35483055e-02,  5.65297921e-02,  3.64892495e-03,\n",
              "         3.05787929e-02,  4.91684778e-03,  4.24009670e-01,\n",
              "         2.14741750e-02,  7.39929164e-02,  1.49993296e-02,\n",
              "         6.85882273e-02,  4.62870298e-04,  8.67027370e-03,\n",
              "         8.26830454e-01, -1.05087298e-01, -5.77013755e-03,\n",
              "         2.91947095e-02,  1.14050148e-02, -6.71869154e-02,\n",
              "         1.74922535e-02, -1.88439616e-01,  1.57712580e-02,\n",
              "        -4.16502128e-03,  2.65939515e-02,  1.91112106e-02,\n",
              "         3.96565940e-03, -3.10227582e-01, -1.69588739e-01,\n",
              "         2.29527198e-03, -7.87516996e-02,  3.51523846e-03,\n",
              "        -7.66296552e-02,  7.28619801e-03, -4.62884439e-02,\n",
              "         9.65028729e-02, -3.45704839e-03, -1.08523869e-03,\n",
              "         1.25419116e-01, -1.36696567e-02,  7.92458753e-03,\n",
              "         1.49984244e-01,  6.26335596e-02,  8.45956428e-03,\n",
              "         3.35751183e-02, -3.85695150e-03, -1.60135165e-01,\n",
              "        -5.72022267e-02, -3.19227025e-02, -7.39960976e-02,\n",
              "        -5.16696092e-03,  5.96615291e-02,  4.92601006e-02,\n",
              "        -4.43590944e-02,  2.32398208e-02,  1.90796633e-02,\n",
              "        -9.35853771e-02,  2.23725427e-03,  5.81742661e-02,\n",
              "         2.70742213e-02,  1.10226423e-01, -6.86166805e-03,\n",
              "        -5.65296321e-02,  1.07259173e-02, -9.05801736e-02,\n",
              "        -2.75265160e-03, -5.60287918e-02,  9.17319106e-03]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# with the parameter coef_ we access the coefficients of the variables\n",
        "# for the linear regression (for all the 108 variables)\n",
        "\n",
        "sel_.estimator_.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xeg-8x-J-EBq",
        "outputId": "bd297b01-f025-4202-86ea-2a39b19a8c71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01202180725756966"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# as SelectFromModel selects coefficients above the mean\n",
        "# of all coefficients, let's calculate first the mean\n",
        "\n",
        "sel_.estimator_.coef_.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Mo2IrDES-EBq",
        "outputId": "d380bcba-803a-4855-86e0-8c965ded218d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# and now let's plot the distribution of coefficients\n",
        "\n",
        "pd.Series(sel_.estimator_.coef_.ravel()).hist(bins=20)\n",
        "plt.xlabel('Coefficients')\n",
        "plt.ylabel('Number of variables')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GlS_bby-EBr"
      },
      "source": [
        "As expected, some coefficients are positive and some are negative, suggesting that some features are negatively associated with the outcome (the more of the feature the less of the outcome) and viceversa.\n",
        "\n",
        "However, the absolute value of the coefficients inform about the importance of the feature on the outcome, and not the sign. Therefore, the feature selection is done filtering on absolute values of coefficients. See below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "xjvwMZlN-EBr",
        "outputId": "afaf3971-2240-4744-8d8c-0868f2226e73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.056882749087269736"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# the feature importance is informed by the absolute value of\n",
        "# the coefficient, and not the sign.\n",
        "# therefore, let's recalculate the mean using the absolute values instead\n",
        "\n",
        "np.abs(sel_.estimator_.coef_).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "vN0FNLGD-EBr",
        "outputId": "517772e3-692d-4258-bd0d-ab9e8603f39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# and now let's plot the histogram of absolute coefficients\n",
        "\n",
        "pd.Series(np.abs(sel_.estimator_.coef_).ravel()).hist(bins=20)\n",
        "plt.xlabel('Coefficients')\n",
        "plt.ylabel('Number of variables')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "tR3fmWJC-EBr",
        "outputId": "11c04dfe-b943-482f-b206-1cb48d532434",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total features: 108\n",
            "selected features: 33\n",
            "features with coefficients greater than the mean coefficient: 33\n"
          ]
        }
      ],
      "source": [
        "# and now, let's compare the  number of selected features\n",
        "# with the number of features which coefficient is above the\n",
        "# mean coefficient, to make sure we understand the output of\n",
        "# SelectFromModel\n",
        "\n",
        "print('total features: {}'.format((X_train.shape[1])))\n",
        "\n",
        "print('selected features: {}'.format(len(selected_feat)))\n",
        "\n",
        "print(\n",
        "    'features with coefficients greater than the mean coefficient: {}'.format(\n",
        "        np.sum(\n",
        "            np.abs(sel_.estimator_.coef_) > np.abs(\n",
        "                sel_.estimator_.coef_).mean())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUA-vBtg-EBr"
      },
      "source": [
        "Therefore, we see how select from model works. It will select all the coefficients which absolute values are greater than the mean.\n",
        "\n",
        "To do this, you need to change the default value of the parameter threshold that can be passed to SelectFromModel. More details in the Scikit-learn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html\n",
        "\n",
        "This is all for this lecture. See you in the next one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mq8vLqaI-EBr"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "fsml",
      "language": "python",
      "name": "fsml"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": "block",
      "toc_window_display": true
    },
    "colab": {
      "name": "08.1-Logistic-regression-coefficients.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}